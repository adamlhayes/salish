from bs4 import BeautifulSoup
import requests
import os
import pandas as pd
import csv

output_file = open("all hyperlinks.csv", "a", encoding="utf-8")

direct = "wget level_1 rerun"
sub1 = "www.washingtontrollers.org"


for root, dirs, files in os.walk(os.path.join(direct, sub1)):
    for name in files: 
        my_file = open(os.path.join(root, name), 'r', encoding="utf8", errors="ignore")
        page_text = my_file.read()   
        soup = BeautifulSoup(page_text.encode('UTF-8'), 'lxml')
        for line in soup.find_all("a"):
            href = str(line.get('href'))
            if href.startswith("http", 0, 4):
                if "twitter" not in href and "facebook" not in href and "vimeo" not in href and "instagram" not in href and "flickr" not in href and "youtube" not in href and "linkedin" not in href:
                    output = "http://" + sub1 + "," + href
                    print(output)
                    print(output, file = output_file)
