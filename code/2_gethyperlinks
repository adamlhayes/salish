from bs4 import BeautifulSoup
import requests
import os
import pandas as pd
import csv
pip install chardet

output_file = open("all hyperlinks.csv", "w", encoding="utf-8")

with open('links_folder list.txt', 'r') as f:
    seeds = f.readlines()
    
urls = []
for oldname in seeds:
    newname = oldname.strip('\n')
    urls.append(newname)
print(urls)    


direct = "wget level_1"

for url in urls:
    filename = url
    print(filename)
    sep1 = "://"
    filename = filename.split(sep1, 1)[1]
    print(filename)
    for root, dirs, files in os.walk(os.path.join(direct, filename)):
        for name in files:
            print(os.path.join(root, name))    
            my_file = open(os.path.join(root, name), 'r', encoding="utf8", errors="ignore")
            page_text = my_file.read()   
            soup = BeautifulSoup(page_text.encode('UTF-8'), 'lxml')
            for line in soup.find_all("a"):
                href = str(line.get('href'))
                if href.startswith("http", 0, 4):
                    if "twitter" not in href and "facebook" not in href and "vimeo" not in href and "instagram" not in href and "flickr" not in href and "youtube" not in href and "linkedin" not in href:
                        output = url + "," + name + "," + href
                        print(output, file = output_file)
